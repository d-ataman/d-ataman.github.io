---
permalink: /
title: "Duygu Ataman, Ph.D."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an incoming Faculty Fellow and Assistant Professor at the Courant Institute of Mathematical Sciences, New York University. I work on developing novel <strong>machine learning methods for natural language processing</strong>, specifically in generative tasks like machine translation. I am also interested in developing corpora in low-resource languages and dialects.

The main goal of my research is to improve the applicability of language technologies in low-resource languages and tasks. Language technology is a highly promising tool with the potential of transforming how we use and benefit from education and the media, although state-of-the-art approaches are still not competitive enough to be deployed in a large portion of the world languages. From a socioeconomic perspective, this includes languages spoken in geographically under-developed parts with few written (or spoken) resources to build natural language processing systems. Linguistically most of these languages happen to have rich morphology which also creates the data sparsity problem. On the other hand, from the machine learning perspective, this problem is equivalent to learning to generalize from minimal data. I find this strictly constrained design problem quite intriguing due to its potential in benefiting a tremendous amount of future applications; at the same time, a challenging engineering task that aligns well with my background in electrical engineering, where I had specialised on developing optimized software solutions for real-time computation-intensive multimedia technology. 

In the context of natural language processing, currently the most promising approach to improve the performance in low-resource languages and tasks are multi-lingual learning methods. I am especially interested in unsupervised representation learning methods based on <strong>latent-variable</strong> and <strong>geometric deep learning models</strong> which can be useful for inducing linguistic biases into the learning architectures. In order to accelerate research in this field I recently initiated the organization of the [1st Workshop on Multilingual Representation Learning](https://sites.google.com/view/mrl-2021), which will take place in EMNLP 2021. I am also part of the [TIL (Turkic Interlingua)](https://turkic-interlingua.org/) community which aims to advance language technology in Turkic languages by developing open-source data sets and tools.

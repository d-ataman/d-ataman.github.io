---
permalink: /
title: "Duygu Ataman, Ph.D."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a post-doctoral researcher at the [Institute of Computational Linguistics, University of ZÃ¼rich](https://www.cl.uzh.ch/de/people/team/compling/ataman.html) and a lecturer at the bachelor's studies in Computational Linguistics and Language Technology. I work on developing novel <strong>machine learning methods for natural language processing</strong>, specifically in generative tasks like machine translation as well as automatic linguistic analysis tools such as morphological analysis and part-of-speech tagging under <strong>low-resource settings</strong>. I also take part in initiatives to develop corpora in low-resource languages and dialects.

The main goal of my research is to improve the applicability of language technologies in low-resource languages and tasks. Language technology is a highly promising tool with the potential of transforming how we use and benefit from education, media and business, although the state-of-the-art approaches are still not competitive enough to be deployed in a large portion of the world. From a socioeconomic perspective, this includes languages spoken in geographically under-developed parts with few to no online written or spoken resources to build natural language processing systems. Linguistically these languages mostly intersect with morphologically-rich languages which also have the data sparsity problem. On the other hand, from the machine learning perspective, this problem is equivalent to learning to generalize from minimal data. I find this strictly constrained design problem quite intriguing due to its potential in benefiting a tremendous amount of future applications. At the same time, a challenging engineering task that aligns well with my background in electrical engineering, where I had specialised on developing optimized software solutions for real-time computation-intensive multimedia technology, such as computer vision, signal processing and secure signal transmission.

In the context of natural language processing, currently the most promising approach to improve the performance in low-resource languages and tasks are multi-lingual and multi-task learning methods. This is why I joined the [MUTAMUR](https://www.cl.uzh.ch/en/texttechnologies/research/machine-learning/mutamur.html) project of [Prof. Dr. Rico Sennrich](https://www.cl.uzh.ch/de/people/team/compling/sennrich.html) which aims to investigate methods for knowledge sharing and transfer between machine learning models in natural language processing. In order to accelerate research in this field I recently initiated the organization of the first workshop on Multilingual Representation Learning, which will take place in EMNLP 2021. 

I am especially interested in unsupervised learning methods based on <strong>Bayesian latent-variable</strong> and <strong>geometric deep learning models</strong> that can express universal structures in morphology and syntax, which could be used to induce linguistic biases into multi-lingual language models and aid their generalization. 











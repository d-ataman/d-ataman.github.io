---
permalink: /
title: "Duygu Ataman, Ph.D."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a post-doctoral researcher at the [Institute of Computational Linguistics, University of ZÃ¼rich](https://www.cl.uzh.ch/de/people/team/compling/ataman.html) and a lecturer at the bachelor's studies in Computational Linguistics and Language Technology. I work on developing novel <strong>machine learning methods for natural language processing</strong>, specifically in generative tasks like machine translation. I am also interested in developing corpora in low-resource languages and dialects.

The main goal of my research is to improve the applicability of language technologies in low-resource languages and tasks. Language technology is a highly promising tool with the potential of transforming how we use and benefit from education and the media, although state-of-the-art approaches are still not competitive enough to be deployed in a large portion of the world languages. From a socioeconomic perspective, this includes languages spoken in geographically under-developed parts with few written (or spoken) resources to build natural language processing systems. Linguistically most of these languages happen to have rich morphology which also creates the data sparsity problem. On the other hand, from the machine learning perspective, this problem is equivalent to learning to generalize from minimal data. I find this strictly constrained design problem quite intriguing due to its potential in benefiting a tremendous amount of future applications; at the same time, a challenging engineering task that aligns well with my background in electrical engineering, where I had specialised on developing optimized software solutions for real-time computation-intensive multimedia technology. 

In the context of natural language processing, currently the most promising approach to improve the performance in low-resource languages and tasks are multi-lingual and multi-task learning methods. This is why I joined the [MUTAMUR](https://www.cl.uzh.ch/en/texttechnologies/research/machine-learning/mutamur.html) project of [Prof. Dr. Rico Sennrich](https://www.cl.uzh.ch/de/people/team/compling/sennrich.html) which aims to investigate methods for knowledge sharing between natural language processing systems. I am especially interested in <strong>multi-task</strong> and <strong>unsupervised representation learning</strong> methods based on <strong>Bayesian latent-variable</strong> and <strong>geometric deep learning models</strong> that can be used to induce linguistic biases into the learning architectures. In order to accelerate research in this field I recently initiated the organization of the first workshop on Multilingual Representation Learning, which will take place in EMNLP 2021. I am also part of the [TIL (Turkic Interlingua)](https://turkic-interlingua.org/) community which aims to advance language technology in Turkic languages by developing open-source data sets and tools.

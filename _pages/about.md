---
permalink: /
title: "Duygu Ataman, Ph.D."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a post-doctoral researcher at the Institute of Computational Linguistics, University of ZÃ¼rich and a lecturer at the bachelor's studies in Computational Linguistics and Language Technology. I work on developing novel machine learning methods for natural language processing, specifically in generative tasks as well as automatic linguistic analysis tools such as morphological analysis and Part-of-Speech tagging under low-resource settings. I also take part in initiatives to develop corpora in low-resource languages and dialects.

The main goal of my research is to improve the applicability of language technologies in low-resource languages and tasks. Language technology is a highly promising tool with the potential of transforming how we use and benefit from education, media and business, although the state-of-the-art approaches are still not competitive enough to be deployed in a large portion of the world. From a socioeconomic perspective, this includes languages spoken in geographically under-developed parts with few to no online written or spoken resources to build natural language processing systems. Linguistically these languages mostly intersect with morphologically-rich languages which also have the data sparsity problem. On the other hand, from the machine learning perspective, this problem is equivalent to learning to generalize from minimal data. I find this strictly constrained design problem quite intriguing due to its potential in benefiting a tremendous amount of future applications. At the same time, a challenging engineering task that aligns well with my background in electrical engineering, where I had specialised on developing optimized software solutions for real-time computation-intensive multimedia technology, such as computer vision, signal processing and secure signal transmission.

In the context of natural language processing, currently the most promising approach to improve the performance in low-resource languages and tasks are multi-lingual and multi-task learning methods. This is why I joined the MUTAMUR project of Prof. Dr. Rico Sennrich which aims to investigate methods for knowledge sharing and transfer between machine learning models in natural language processing. In order to accelerate research in this field I recently initiated the organization of the first workshop on Multilingual Representation Learning, which will take place in EMNLP 2021. 

In addition to investigating multi-lingual learning methods, I am also interested in Bayesian approaches which are especially useful in applications which require unsupervised or semi-supervised learning under low-resource settings. So in a parallel line of work I study novel methods for designing latent-variable or geometric models that can help induce linguistic biases into statistical language modeling architectures. 










